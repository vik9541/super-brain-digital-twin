# üíæ –û–ë–ù–û–í–õ–ï–ù–ò–ï –¢–ó: RAW DATA STORAGE + BATCH ANALYZER

**–î–∞—Ç–∞:** 10 –¥–µ–∫–∞–±—Ä—è 2025, 20:00 MSK  
**–í–µ—Ä—Å–∏—è –¢–ó:** 4.2 (Critical Data Storage Update)  
**–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** üî¥ –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô

---

## üéØ –ü–†–ò–ù–¶–ò–ü: –°–û–•–†–ê–ù–Ø–¢–¨ –í–°–Å ‚Üí –°–¢–†–£–ö–¢–£–†–ò–†–û–í–ê–¢–¨ –ü–û–¢–û–ú

**–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ:** –ë–æ—Ç –¥–æ–ª–∂–µ–Ω —Å–æ—Ö—Ä–∞–Ω—è—Ç—å **–ê–ë–°–û–õ–Æ–¢–ù–û –í–°–Å** –≤ —Å—ã—Ä–æ–º –≤–∏–¥–µ (raw tables), –∞ –Ω–æ—á–Ω–æ–π batch-analyzer –±—É–¥–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞—Ç—å –∏ —Ä–∞—Å–∫–ª–∞–¥—ã–≤–∞—Ç—å –ø–æ —Ç–∞–±–ª–∏—Ü–∞–º.

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:**
1. **BOT (Real-time)** ‚Üí –°–æ—Ö—Ä–∞–Ω—è–µ—Ç **–í–°–Å** –≤ raw tables
2. **BATCH ANALYZER (03:00 MSK)** ‚Üí –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–µ—Ç –∏ —Ä–∞—Å–∫–ª–∞–¥—ã–≤–∞–µ—Ç
3. **–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫** ‚Üí Batch analyzer –ø–µ—Ä–µ–º–µ—â–∞–µ—Ç –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ —Ä–∞–∑–ª–æ–∂–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ

---

## üìä –ù–û–í–´–ï –¢–ê–ë–õ–ò–¶–´ SUPABASE

### 1. raw_messages ‚Äî –°—ã—Ä—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è

```sql
CREATE TABLE raw_messages (
  id BIGSERIAL PRIMARY KEY,
  user_id BIGINT NOT NULL,
  message_id BIGINT NOT NULL,
  message_text TEXT,
  message_type TEXT NOT NULL,
  
  -- –¶–ï–ü–û–ß–ö–ê –û–¢–í–ï–¢–û–í (reply chain)
  reply_to_message_id BIGINT,
  reply_to_text TEXT,
  is_clarification BOOLEAN DEFAULT FALSE,
  
  -- –°—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ
  raw_telegram_json JSONB,
  
  -- –°—Ç–∞—Ç—É—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏
  is_processed BOOLEAN DEFAULT FALSE,
  processed_at TIMESTAMP,
  created_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_raw_messages_user ON raw_messages(user_id);
CREATE INDEX idx_raw_messages_processed ON raw_messages(is_processed);
```

### 2. bot_responses ‚Äî –û—Ç–≤–µ—Ç—ã –±–æ—Ç–∞

```sql
CREATE TABLE bot_responses (
  id BIGSERIAL PRIMARY KEY,
  user_id BIGINT NOT NULL,
  message_id BIGINT NOT NULL,
  response_text TEXT NOT NULL,
  response_type TEXT,
  related_user_message_id BIGINT,
  ai_analysis JSONB,
  chain_id UUID,
  created_at TIMESTAMP DEFAULT NOW()
);
```

### 3. raw_files ‚Äî –°—ã—Ä—ã–µ —Ñ–∞–π–ª—ã

```sql
CREATE TABLE raw_files (
  id BIGSERIAL PRIMARY KEY,
  user_id BIGINT NOT NULL,
  message_id BIGINT NOT NULL,
  file_type TEXT NOT NULL,
  file_name TEXT,
  file_path TEXT NOT NULL,
  file_size BIGINT,
  file_hash TEXT UNIQUE,
  raw_metadata JSONB,
  is_processed BOOLEAN DEFAULT FALSE,
  processed_at TIMESTAMP,
  uploaded_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_raw_files_user ON raw_files(user_id);
CREATE INDEX idx_raw_files_hash ON raw_files(file_hash);
```

### 4. message_chains ‚Äî –¶–µ–ø–æ—á–∫–∏ –¥–∏–∞–ª–æ–≥–æ–≤

```sql
CREATE TABLE message_chains (
  id BIGSERIAL PRIMARY KEY,
  chain_id UUID UNIQUE DEFAULT gen_random_uuid(),
  user_id BIGINT NOT NULL,
  original_message_id BIGINT NOT NULL,
  original_text TEXT,
  replies JSONB DEFAULT '[]'::jsonb,
  created_at TIMESTAMP DEFAULT NOW(),
  updated_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_chains_user ON message_chains(user_id);
```

---

## üîß –û–ë–ù–û–í–õ–ï–ù–ò–ï bot_handler.py

**–î–æ–±–∞–≤–∏—Ç—å –ø–æ—Å–ª–µ —Å—Ç—Ä–æ–∫–∏ 70 –≤ `handle_universal_message`:**

```python
# ====== –ù–û–í–û–ï: –û–ë–†–ê–ë–û–¢–ö–ê REPLY CHAIN ======
reply_to_text = None
reply_to_message_id = None
is_clarification = False

if message.reply_to_message:
    reply_to_text = message.reply_to_message.text or "[–ú–µ–¥–∏–∞ –∫–æ–Ω—Ç–µ–Ω—Ç]"
    reply_to_message_id = message.reply_to_message.message_id
    is_clarification = True
    logger.info(f"üìå –û–±–Ω–∞—Ä—É–∂–µ–Ω –æ—Ç–≤–µ—Ç –Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏–µ #{reply_to_message_id}")

# ====== –°–û–•–†–ê–ù–ï–ù–ò–ï –í –°–´–†–£–Æ –¢–ê–ë–õ–ò–¶–£ ======
from datetime import datetime
from supabase import create_client

supabase_url = os.getenv("SUPABASE_URL")
supabase_key = os.getenv("SUPABASE_KEY")
supabase = create_client(supabase_url, supabase_key)

raw_message_data = {
    "user_id": user_id,
    "message_id": message.message_id,
    "message_text": message_text,
    "message_type": message_type,
    "reply_to_message_id": reply_to_message_id,
    "reply_to_text": reply_to_text,
    "is_clarification": is_clarification,
    "raw_telegram_json": message.model_dump() if hasattr(message, 'model_dump') else {},
    "created_at": datetime.now().isoformat()
}

try:
    supabase.table("raw_messages").insert(raw_message_data).execute()
    logger.info("‚úÖ –°–æ–æ–±—â–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ raw_messages")
except Exception as e:
    logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")

# ====== –û–ë–ù–û–í–õ–ï–ù–ò–ï –¶–ï–ü–û–ß–ö–ò ======
if is_clarification:
    try:
        chain = supabase.table("message_chains").select("*").eq("original_message_id", reply_to_message_id).execute()
        
        if not chain.data:
            chain_data = {
                "user_id": user_id,
                "original_message_id": reply_to_message_id,
                "original_text": reply_to_text,
                "replies": [{
                    "reply_message_id": message.message_id,
                    "reply_text": message_text,
                    "timestamp": datetime.now().isoformat()
                }]
            }
            supabase.table("message_chains").insert(chain_data).execute()
        else:
            chain_id = chain.data[0]["id"]
            replies = chain.data[0]["replies"] or []
            replies.append({
                "reply_message_id": message.message_id,
                "reply_text": message_text,
                "timestamp": datetime.now().isoformat()
            })
            supabase.table("message_chains").update({"replies": replies}).eq("id", chain_id).execute()
        
        logger.info("‚úÖ –¶–µ–ø–æ—á–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∞")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ü–µ–ø–æ—á–∫–∏: {e}")

# –î–æ–±–∞–≤–∏—Ç—å –≤ analysis_data
analysis_data["reply_to_message"] = reply_to_text
analysis_data["is_clarification"] = is_clarification
```

---

## üåô BATCH ANALYZER - –ù–æ—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞

**–§–∞–π–ª:** `batch_analyzer.py`

```python
"""üåô Batch Analyzer - –ù–æ—á–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
–ó–∞–ø—É—Å–∫: –ö–∞–∂–¥—É—é –Ω–æ—á—å –≤ 03:00 MSK
"""
import asyncio
import os
import logging
from datetime import datetime
from supabase import create_client
import httpx

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")
N8N_WEBHOOK = os.getenv("N8N_WEBHOOK_BASE", "https://lavrentev.app.n8n.cloud/webhook")

supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

async def process_raw_messages():
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—ã—Ä—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π"""
    logger.info("üì• –ó–∞–≥—Ä—É–∑–∫–∞ raw_messages...")
    
    raw = supabase.table("raw_messages").select("*").eq("is_processed", False).execute()
    logger.info(f"üìä –ù–∞–π–¥–µ–Ω–æ: {len(raw.data)} —Å–æ–æ–±—â–µ–Ω–∏–π")
    
    for msg in raw.data:
        try:
            # –ê–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ Perplexity AI
            async with httpx.AsyncClient(timeout=30) as client:
                response = await client.post(f"{N8N_WEBHOOK}/digital-twin-ask", json={
                    "message": msg["message_text"],
                    "message_type": msg["message_type"],
                    "user_id": msg["user_id"],
                    "reply_to_message": msg.get("reply_to_text"),
                    "is_clarification": msg.get("is_clarification", False)
                })
                analysis = response.json()
            
            # –†–∞—Å–∫–ª–∞–¥—ã–≤–∞–µ–º –ø–æ —Ç–∞–±–ª–∏—Ü–∞–º
            if analysis.get("type") == "file":
                # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ files
                pass
            elif analysis.get("type") == "event":
                # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ events
                pass
            
            # –ü–æ–º–µ—á–∞–µ–º –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–µ
            supabase.table("raw_messages").update({
                "is_processed": True,
                "processed_at": datetime.now().isoformat()
            }).eq("id", msg["id"]).execute()
            
            logger.info(f"‚úÖ –°–æ–æ–±—â–µ–Ω–∏–µ #{msg['id']} –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ #{msg['id']}: {e}")

async def main():
    logger.info(f"üåô Batch Analyzer –∑–∞–ø—É—â–µ–Ω: {datetime.now()}")
    await process_raw_messages()
    logger.info("‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ")

if __name__ == "__main__":
    asyncio.run(main())
```

---

## ‚ò∏Ô∏è K8S CRONJOB

**–§–∞–π–ª:** `k8s/batch-analyzer-cronjob.yaml`

```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: batch-analyzer
  namespace: super-brain
spec:
  schedule: "0 3 * * *"  # –ö–∞–∂–¥—É—é –Ω–æ—á—å –≤ 03:00 MSK
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: batch-analyzer
            image: registry.digitalocean.com/digital-twin-registry/batch-analyzer:latest
            env:
            - name: SUPABASE_URL
              valueFrom:
                secretKeyRef:
                  name: supabase-credentials
                  key: url
            - name: SUPABASE_KEY
              valueFrom:
                secretKeyRef:
                  name: supabase-credentials
                  key: key
            - name: N8N_WEBHOOK_BASE
              value: "https://lavrentev.app.n8n.cloud/webhook"
          restartPolicy: OnFailure
```

---

## ‚úÖ –ß–ï–ö–õ–ò–°–¢ –í–´–ü–û–õ–ù–ï–ù–ò–Ø

- [ ] –°–æ–∑–¥–∞—Ç—å —Ç–∞–±–ª–∏—Ü—ã –≤ Supabase (SECURE_SCHEMA_V3.sql)
- [ ] –û–±–Ω–æ–≤–∏—Ç—å bot_handler.py (–æ–±—Ä–∞–±–æ—Ç–∫–∞ reply_to_message)
- [ ] –°–æ–∑–¥–∞—Ç—å batch_analyzer.py
- [ ] –°–æ–∑–¥–∞—Ç—å Dockerfile.batch-analyzer
- [ ] –°–æ–∑–¥–∞—Ç—å batch-analyzer-cronjob.yaml
- [ ] –û–±–Ω–æ–≤–∏—Ç—å SUPER_BRAIN_FLEXIBLE_TZ_v4.0.md
- [ ] –ó–∞–ø—É—Å—Ç–∏—Ç—å GitHub Actions (Build and Push)
- [ ] –ü—Ä–∏–º–µ–Ω–∏—Ç—å kubectl apply -f k8s/

---

**–î–∞—Ç–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è:** 10 –¥–µ–∫–∞–±—Ä—è 2025, 20:00 MSK  
**–°—Ç–∞—Ç—É—Å:** üü° –ì–û–¢–û–í–û –ö –í–´–ü–û–õ–ù–ï–ù–ò–Æ  
**–ê–≤—Ç–æ—Ä:** Perplexity AI + vik9541
